{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lines without 'CID' have been written to /Users/kavithakamarthy/Downloads/SSGU-CD-all/SSGU-CD/dataset/cdr/CDR_TestSet1.PubTator.txt\n"
     ]
    }
   ],
   "source": [
    "# Define the input and output file paths\n",
    "input_file = \"/Users/kavithakamarthy/Downloads/SSGU-CD-all/SSGU-CD/dataset/cdr/CDR_TestSet.PubTator.txt\"  # Replace with your input file path\n",
    "output_file = \"/Users/kavithakamarthy/Downloads/SSGU-CD-all/SSGU-CD/dataset/cdr/CDR_TestSet1.PubTator.txt\"  # Replace with your desired output file path\n",
    "\n",
    "# Open the input file and filter out lines containing \"CID\"\n",
    "with open(input_file, \"r\") as infile, open(output_file, \"w\") as outfile:\n",
    "    for line in infile:\n",
    "        if \"\\tCID\\t\" not in line:  # Check if \"CID\" is not in the line\n",
    "            outfile.write(line)  # Write the line to the output file if it doesn't contain \"CID\"\n",
    "\n",
    "print(f\"Lines without 'CID' have been written to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kavithakamarthy/.pyenv/versions/3.11.9/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(task='cdr', data_dir='./dataset/cdr', transformer_type='bert', model_name_or_path='/Users/kavithakamarthy/Downloads/SSGU-CD/dataset/cdr/data/pretrained/scibert_scivocab_cased', train_file='Train.BioC.JSON', dev_file='Dev.BioC.JSON', test_file='test.data', save_path='out/Train_bert_cdr_seed_BSCELoss_tree_0.3_0.5_66', load_path='', config_name='', tokenizer_name='', max_seq_length=1024, train_batch_size=12, test_batch_size=12, gradient_accumulation_steps=1, learning_rate=2e-05, adam_epsilon=1e-06, max_grad_norm=1.0, warmup_ratio=0.06, num_train_epochs=30, evaluation_steps=-1, seed=66, num_class=2, gnn='GCN', use_gcn='tree', dropout=0.5, loss='BSCELoss', s0=0.3, demo='false', unet_in_dim=3, unet_out_dim=256, down_dim=256, bert_lr=3e-05, max_height=64, rel2=0, save_result='', save_pubtator='./result/cdr/cdr_BSCELoss_tree_s0=0.3_dropout=0.5_66')\n",
      "generate predict result in ./result/cdr/cdr_BSCELoss_tree_s0=0.3_dropout=0.5_66\n",
      "./result/cdr/cdr_BSCELoss_tree_s0=0.3_dropout=0.5_66.pubtator\n",
      "{'test_F1': 0.0, 'test_P': 0.0, 'test_R': 0.0}\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import sys\n",
    "import os\n",
    "import torch\n",
    "from transformers import AutoConfig, AutoModel, AutoTokenizer\n",
    "from model_bio import Model\n",
    "from utils import set_seed\n",
    "from prepro_bio import read_bio\n",
    "from save_result import Logger\n",
    "from evaluation import train, evaluate, to_official_bio, gen_data_bio\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--task\", default=\"cdr\", type=str)\n",
    "parser.add_argument(\"--data_dir\", default=\"./dataset/cdr\", type=str)\n",
    "parser.add_argument(\"--transformer_type\", default=\"bert\", type=str)\n",
    "parser.add_argument(\"--model_name_or_path\", default=\"bert-base-cased\", type=str)\n",
    "parser.add_argument(\"--train_file\", default=\"Train.BioC.JSON\", type=str)\n",
    "parser.add_argument(\"--dev_file\", default=\"Dev.BioC.JSON\", type=str)\n",
    "parser.add_argument(\"--test_file\", default=\"Test.BioC.JSON\", type=str)\n",
    "parser.add_argument(\"--save_path\", default=\"out\", type=str)\n",
    "parser.add_argument(\"--load_path\", default=\"\", type=str)\n",
    "parser.add_argument(\"--config_name\", default=\"\", type=str,\n",
    "                    help=\"Pretrained config name or path if not the same as model_name\")\n",
    "parser.add_argument(\"--tokenizer_name\", default=\"\", type=str,\n",
    "                    help=\"Pretrained tokenizer name or path if not the same as model_name\")\n",
    "parser.add_argument(\"--max_seq_length\", default=1024, type=int,\n",
    "                    help=\"The maximum total input sequence length after tokenization. Sequences longer \"\n",
    "                         \"than this will be truncated, sequences shorter will be padded.\")\n",
    "parser.add_argument(\"--train_batch_size\", default=4, type=int, help=\"Batch size for training.\")\n",
    "parser.add_argument(\"--test_batch_size\", default=8, type=int, help=\"Batch size for testing.\")\n",
    "parser.add_argument(\"--gradient_accumulation_steps\", default=1, type=int,\n",
    "                    help=\"Number of updates steps to accumulate before performing a backward/update pass.\")\n",
    "parser.add_argument(\"--learning_rate\", default=5e-5, type=float, help=\"The initial learning rate for Adam.\")\n",
    "parser.add_argument(\"--adam_epsilon\", default=1e-6, type=float, help=\"Epsilon for Adam optimizer.\")\n",
    "parser.add_argument(\"--max_grad_norm\", default=1.0, type=float, help=\"Max gradient norm.\")\n",
    "parser.add_argument(\"--warmup_ratio\", default=0.06, type=float, help=\"Warm up ratio for Adam.\")\n",
    "parser.add_argument(\"--num_train_epochs\", default=30.0, type=float, help=\"Total number of training epochs to perform.\")\n",
    "parser.add_argument(\"--evaluation_steps\", default=-1, type=int, help=\"Number of training steps between evaluations.\")\n",
    "parser.add_argument(\"--seed\", type=int, default=66, help=\"random seed for initialization\")\n",
    "parser.add_argument(\"--num_class\", type=int, default=97, help=\"Number of relation types in dataset.\")\n",
    "parser.add_argument('--gnn', type=str, default='GCN', help=\"GCN/GAT\")\n",
    "parser.add_argument('--use_gcn', type=str, default='tree', help=\"use gcn, both/mentions/tree/false\")\n",
    "parser.add_argument('--dropout', type=float, default=0.5, help=\"0.0/0.2/0.5\")\n",
    "parser.add_argument('--loss', type=str, default='BSCELoss',\n",
    "                    help=\"use BSCELoss/BalancedLoss/ATLoss/AsymmetricLoss/APLLoss\")\n",
    "parser.add_argument('--s0', type=float, default=0.3)\n",
    "parser.add_argument(\"--demo\", type=str, default='false', help='use a few data to test. default true/false')\n",
    "parser.add_argument(\"--unet_in_dim\", type=int, default=3, help=\"unet_in_dim.\")\n",
    "parser.add_argument(\"--unet_out_dim\", type=int, default=256, help=\"unet_out_dim.\")\n",
    "parser.add_argument(\"--down_dim\", type=int, default=256, help=\"down_dim.\")\n",
    "parser.add_argument(\"--bert_lr\", default=3e-5, type=float, help=\"The initial learning rate for Adam.\")\n",
    "parser.add_argument(\"--max_height\", type=int, default=64, help=\"max_height.\")\n",
    "parser.add_argument(\"--rel2\", type=int, default=0, help=\"\")\n",
    "parser.add_argument(\"--save_result\", type=str, default=\"\", help=\"save predict result.\")\n",
    "args, _ = parser.parse_known_args()\n",
    "\n",
    "if args.task == 'cdr':\n",
    "    args.data_dir = './dataset/cdr'\n",
    "    # args.train_file = 'train_filter.data'\n",
    "    # args.dev_file = 'dev_filter.data'\n",
    "    args.test_file = 'test.data'\n",
    "    args.model_name_or_path = '/Users/kavithakamarthy/Downloads/SSGU-CD/dataset/cdr/data/pretrained/scibert_scivocab_cased'\n",
    "    args.train_batch_size = 12\n",
    "    args.test_batch_size = 12\n",
    "    args.learning_rate = 2e-5\n",
    "    args.num_class = 2\n",
    "    args.num_train_epochs = 30\n",
    "\n",
    "if not os.path.exists(args.save_path):\n",
    "    os.mkdir(args.save_path)\n",
    "file_name = \"{}_{}_{}_seed_{}_{}_{}_{}_{}\".format(\n",
    "    args.train_file.split('.')[0],\n",
    "    args.transformer_type, args.data_dir.split('/')[-1],\n",
    "    args.loss, args.use_gcn, args.s0, args.dropout, str(args.seed), )\n",
    "args.save_path = os.path.join(args.save_path, file_name)\n",
    "args.save_pubtator = os.path.join('./result/' + args.task + '/' + args.task  + '_' + args.loss\n",
    "                                  + '_' + str(args.use_gcn) + '_s0=' + str(args.s0)\n",
    "                                  + '_dropout=' + str(args.dropout) + '_' + str(args.seed))\n",
    "if args.load_path == \"\":\n",
    "    sys.stdout = Logger(stream=sys.stdout, filename=args.save_pubtator + '_test.log')\n",
    "read = read_bio\n",
    "print(args)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "args.n_gpu = torch.cuda.device_count()\n",
    "args.device = device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = AutoConfig.from_pretrained(\n",
    "    args.config_name if args.config_name else args.model_name_or_path, num_labels=args.num_class, )\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    args.tokenizer_name if args.tokenizer_name else args.model_name_or_path, )\n",
    "model = AutoModel.from_pretrained(\n",
    "    args.model_name_or_path, from_tf=bool(\".ckpt\" in args.model_name_or_path), config=config, )\n",
    "config.cls_token_id = tokenizer.cls_token_id\n",
    "config.sep_token_id = tokenizer.sep_token_id\n",
    "config.transformer_type = args.transformer_type\n",
    "set_seed(args)\n",
    "model = Model(args, config, model, num_labels=1)\n",
    "model.to('cpu')\n",
    "model_path = '/Users/kavithakamarthy/Downloads/SSGU-CD-all/SSGU-CD/out/train_filter_bert_cdr_seed_BSCELoss_tree_0.3_0.5_66_best'\n",
    "model.load_state_dict(torch.load(model_path, map_location=torch.device('cpu')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [00:58<00:00,  8.54it/s]\n",
      "BertSdpaSelfAttention is used but `torch.nn.functional.scaled_dot_product_attention` does not support non-absolute `position_embedding_type` or `output_attentions=True` or `head_mask`. Falling back to the manual attention implementation, but specifying the manual implementation will be required from Transformers version v5.0.0 onwards. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.\n"
     ]
    }
   ],
   "source": [
    "test_file = os.path.join(args.data_dir, args.test_file)\n",
    "test_features = read(args, test_file, tokenizer, max_seq_length=args.max_seq_length)\n",
    "test_score, test_output, preds = evaluate(args, model, test_features, tag=\"test\", generate=True)\n",
    "print(test_output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
