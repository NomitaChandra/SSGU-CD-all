{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train on the CDR dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [00:48<00:00, 10.40it/s]\n",
      "100%|██████████| 500/500 [00:54<00:00,  9.25it/s]\n",
      "100%|██████████| 500/500 [01:08<00:00,  7.28it/s]\n",
      "/Users/kavithakamarthy/.pyenv/versions/3.11.9/lib/python3.11/site-packages/transformers/optimization.py:588: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "epoch:   0%|          | 0/30 [00:00<?, ?it/s]BertSdpaSelfAttention is used but `torch.nn.functional.scaled_dot_product_attention` does not support non-absolute `position_embedding_type` or `output_attentions=True` or `head_mask`. Falling back to the manual attention implementation, but specifying the manual implementation will be required from Transformers version v5.0.0 onwards. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.\n",
      "epoch:   0%|          | 0/30 [13:20<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 108\u001b[0m\n\u001b[1;32m    106\u001b[0m dev_features \u001b[38;5;241m=\u001b[39m read(args, dev_file, tokenizer, max_seq_length\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mmax_seq_length)\n\u001b[1;32m    107\u001b[0m test_features \u001b[38;5;241m=\u001b[39m read(args, test_file, tokenizer, max_seq_length\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mmax_seq_length)\n\u001b[0;32m--> 108\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdev_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_features\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    110\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBEST TEST\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    111\u001b[0m model\u001b[38;5;241m.\u001b[39mload_state_dict(torch\u001b[38;5;241m.\u001b[39mload(args\u001b[38;5;241m.\u001b[39msave_path \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_best\u001b[39m\u001b[38;5;124m'\u001b[39m))\n",
      "File \u001b[0;32m~/Downloads/SSGU-CD/evaluation.py:86\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(args, model, train_features, dev_features, test_features)\u001b[0m\n\u001b[1;32m     84\u001b[0m set_seed(args)\n\u001b[1;32m     85\u001b[0m model\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 86\u001b[0m \u001b[43mfinetune\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_train_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_steps\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Downloads/SSGU-CD/evaluation.py:52\u001b[0m, in \u001b[0;36mtrain.<locals>.finetune\u001b[0;34m(features, optimizer, num_epoch, num_steps)\u001b[0m\n\u001b[1;32m     50\u001b[0m outputs[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msum(outputs[\u001b[38;5;241m0\u001b[39m], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     51\u001b[0m loss \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m/\u001b[39m args\u001b[38;5;241m.\u001b[39mgradient_accumulation_steps\n\u001b[0;32m---> 52\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m step \u001b[38;5;241m%\u001b[39m args\u001b[38;5;241m.\u001b[39mgradient_accumulation_steps \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     54\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m args\u001b[38;5;241m.\u001b[39mmax_grad_norm \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.9/lib/python3.11/site-packages/torch/_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    521\u001b[0m     )\n\u001b[0;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.9/lib/python3.11/site-packages/torch/autograd/__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 266\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import sys\n",
    "import os\n",
    "import torch\n",
    "from transformers import AutoConfig, AutoModel, AutoTokenizer\n",
    "from model_bio import Model\n",
    "from utils import set_seed\n",
    "from prepro_bio import read_bio\n",
    "from save_result import Logger\n",
    "from evaluation import train, evaluate\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--task\", default=\"cdr\", type=str)\n",
    "parser.add_argument(\"--data_dir\", default=\"./dataset/cdr\", type=str)\n",
    "parser.add_argument(\"--transformer_type\", default=\"bert\", type=str)\n",
    "parser.add_argument(\"--model_name_or_path\", default=\"bert-base-cased\", type=str)\n",
    "parser.add_argument(\"--train_file\", default=\"Train.BioC.JSON\", type=str)\n",
    "parser.add_argument(\"--dev_file\", default=\"Dev.BioC.JSON\", type=str)\n",
    "parser.add_argument(\"--test_file\", default=\"Test.BioC.JSON\", type=str)\n",
    "parser.add_argument(\"--save_path\", default=\"out\", type=str)\n",
    "parser.add_argument(\"--load_path\", default=\"\", type=str)\n",
    "parser.add_argument(\"--config_name\", default=\"\", type=str,\n",
    "                    help=\"Pretrained config name or path if not the same as model_name\")\n",
    "parser.add_argument(\"--tokenizer_name\", default=\"\", type=str,\n",
    "                    help=\"Pretrained tokenizer name or path if not the same as model_name\")\n",
    "parser.add_argument(\"--max_seq_length\", default=1024, type=int,\n",
    "                    help=\"The maximum total input sequence length after tokenization. Sequences longer \"\n",
    "                         \"than this will be truncated, sequences shorter will be padded.\")\n",
    "parser.add_argument(\"--train_batch_size\", default=4, type=int, help=\"Batch size for training.\")\n",
    "parser.add_argument(\"--test_batch_size\", default=8, type=int, help=\"Batch size for testing.\")\n",
    "parser.add_argument(\"--gradient_accumulation_steps\", default=1, type=int,\n",
    "                    help=\"Number of updates steps to accumulate before performing a backward/update pass.\")\n",
    "parser.add_argument(\"--learning_rate\", default=5e-5, type=float, help=\"The initial learning rate for Adam.\")\n",
    "parser.add_argument(\"--adam_epsilon\", default=1e-6, type=float, help=\"Epsilon for Adam optimizer.\")\n",
    "parser.add_argument(\"--max_grad_norm\", default=1.0, type=float, help=\"Max gradient norm.\")\n",
    "parser.add_argument(\"--warmup_ratio\", default=0.06, type=float, help=\"Warm up ratio for Adam.\")\n",
    "parser.add_argument(\"--num_train_epochs\", default=30.0, type=float, help=\"Total number of training epochs to perform.\")\n",
    "parser.add_argument(\"--evaluation_steps\", default=-1, type=int, help=\"Number of training steps between evaluations.\")\n",
    "parser.add_argument(\"--seed\", type=int, default=66, help=\"random seed for initialization\")\n",
    "parser.add_argument(\"--num_class\", type=int, default=97, help=\"Number of relation types in dataset.\")\n",
    "parser.add_argument('--gnn', type=str, default='GCN', help=\"GCN/GAT\")\n",
    "parser.add_argument('--use_gcn', type=str, default='tree', help=\"use gcn, both/mentions/tree/false\")\n",
    "parser.add_argument('--dropout', type=float, default=0.5, help=\"0.0/0.2/0.5\")\n",
    "parser.add_argument('--loss', type=str, default='BSCELoss',\n",
    "                    help=\"use BSCELoss/BalancedLoss/ATLoss/AsymmetricLoss/APLLoss\")\n",
    "parser.add_argument('--s0', type=float, default=0.3)\n",
    "parser.add_argument(\"--demo\", type=str, default='false', help='use a few data to test. default true/false')\n",
    "parser.add_argument(\"--unet_in_dim\", type=int, default=3, help=\"unet_in_dim.\")\n",
    "parser.add_argument(\"--unet_out_dim\", type=int, default=256, help=\"unet_out_dim.\")\n",
    "parser.add_argument(\"--down_dim\", type=int, default=256, help=\"down_dim.\")\n",
    "parser.add_argument(\"--bert_lr\", default=3e-5, type=float, help=\"The initial learning rate for Adam.\")\n",
    "parser.add_argument(\"--max_height\", type=int, default=64, help=\"max_height.\")\n",
    "parser.add_argument(\"--rel2\", type=int, default=0, help=\"\")\n",
    "parser.add_argument(\"--save_result\", type=str, default=\"\", help=\"save predict result.\")\n",
    "args, _ = parser.parse_known_args()\n",
    "\n",
    "if args.task == 'cdr':\n",
    "    args.data_dir = './dataset/cdr'\n",
    "    args.train_file = 'train_filter.data'\n",
    "    args.dev_file = 'dev_filter.data'\n",
    "    args.test_file = 'test_filter.data'\n",
    "    args.model_name_or_path = '/Users/kavithakamarthy/Downloads/SSGU-CD/dataset/cdr/data/pretrained/scibert_scivocab_cased'\n",
    "    args.train_batch_size = 12\n",
    "    args.test_batch_size = 12\n",
    "    args.learning_rate = 2e-5\n",
    "    args.num_class = 2\n",
    "    args.num_train_epochs = 30\n",
    "\n",
    "if not os.path.exists(args.save_path):\n",
    "    os.mkdir(args.save_path)\n",
    "file_name = \"{}_{}_{}_seed_{}_{}_{}_{}_{}\".format(\n",
    "    args.train_file.split('.')[0],\n",
    "    args.transformer_type, args.data_dir.split('/')[-1],\n",
    "    args.loss, args.use_gcn, args.s0, args.dropout, str(args.seed), )\n",
    "args.save_path = os.path.join(args.save_path, file_name)\n",
    "args.save_pubtator = os.path.join('./result/' + args.task + '/' + args.task  + '_' + args.loss\n",
    "                                  + '_' + str(args.use_gcn) + '_s0=' + str(args.s0)\n",
    "                                  + '_dropout=' + str(args.dropout) + '_' + str(args.seed))\n",
    "if args.load_path == \"\":\n",
    "    sys.stdout = Logger(stream=sys.stdout, filename=args.save_pubtator + '_test.log')\n",
    "read = read_bio\n",
    "print(args)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "args.n_gpu = torch.cuda.device_count()\n",
    "args.device = device\n",
    "\n",
    "config = AutoConfig.from_pretrained(\n",
    "    args.config_name if args.config_name else args.model_name_or_path, num_labels=args.num_class, )\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    args.tokenizer_name if args.tokenizer_name else args.model_name_or_path, )\n",
    "model = AutoModel.from_pretrained(\n",
    "    args.model_name_or_path, from_tf=bool(\".ckpt\" in args.model_name_or_path), config=config, )\n",
    "config.cls_token_id = tokenizer.cls_token_id\n",
    "config.sep_token_id = tokenizer.sep_token_id\n",
    "config.transformer_type = args.transformer_type\n",
    "set_seed(args)\n",
    "model = Model(args, config, model, num_labels=1)\n",
    "model.to('cpu')\n",
    "\n",
    "# Training\n",
    "train_file = os.path.join(args.data_dir, args.train_file)\n",
    "dev_file = os.path.join(args.data_dir, args.dev_file)\n",
    "test_file = os.path.join(args.data_dir, args.test_file)\n",
    "train_features = read(args, train_file, tokenizer, max_seq_length=args.max_seq_length)\n",
    "dev_features = read(args, dev_file, tokenizer, max_seq_length=args.max_seq_length)\n",
    "test_features = read(args, test_file, tokenizer, max_seq_length=args.max_seq_length)\n",
    "train(args, model, train_features, dev_features, test_features)\n",
    "\n",
    "print(\"BEST TEST\")\n",
    "model.load_state_dict(torch.load(args.save_path + '_best'))\n",
    "dev_score, dev_output = evaluate(args, model, dev_features, tag=\"dev\")\n",
    "print(dev_output)\n",
    "test_score, test_output = evaluate(args, model, test_features, tag=\"test\", generate=True)\n",
    "print(test_output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
