{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation on the CDR dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(adam_epsilon=1e-06, bert_lr=3e-05, config_name='', data_dir='./dataset/cdr', demo='false', dev_file='dev_filter.data', down_dim=256, dropout=0.5, evaluation_steps=-1, gnn='GCN', gradient_accumulation_steps=1, learning_rate=2e-05, load_path='./result/train_filter_cdr_tree', loss='BSCELoss', max_grad_norm=1.0, max_height=64, max_seq_length=1024, model_name_or_path='/data/pretrained/scibert_scivocab_cased', num_class=2, num_train_epochs=30, rel2=0, s0=0.3, save_path='out/train_filter_20240818-2107_bert_cdr_seed_BSCELoss_tree_0.3_0.5_66', save_pubtator='./result/cdr/cdr_20240818-2107_BSCELoss_tree_s0=0.3_dropout=0.5_66', save_result='', seed=66, task='cdr', test_batch_size=12, test_file='test_filter.data', tokenizer_name='', train_batch_size=12, train_file='train_filter.data', transformer_type='bert', unet_in_dim=3, unet_out_dim=256, use_gcn='tree', warmup_ratio=0.06)\n",
      "./result/train_filter_cdr_tree\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [00:37<00:00, 13.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CDR TEST\n",
      "generate predict result in ./result/cdr/cdr_rel2_0_test\n",
      "./result/cdr/cdr_rel2_0_test.pubtator\n",
      "{'test_F1': 86.98941783125949, 'test_P': 89.08554484674981, 'test_R': 84.99061833967525}\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import sys\n",
    "import datetime\n",
    "import os\n",
    "import torch\n",
    "from transformers import AutoConfig, AutoModel, AutoTokenizer\n",
    "from model_bio import Model\n",
    "from utils import set_seed\n",
    "from prepro_bio import read_bio\n",
    "from save_result import Logger\n",
    "from evaluation import train, evaluate\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--task\", default=\"cdr\", type=str)\n",
    "parser.add_argument(\"--data_dir\", default=\"./dataset/cdr\", type=str)\n",
    "parser.add_argument(\"--transformer_type\", default=\"bert\", type=str)\n",
    "parser.add_argument(\"--model_name_or_path\", default=\"bert-base-cased\", type=str)\n",
    "parser.add_argument(\"--train_file\", default=\"Train.BioC.JSON\", type=str)\n",
    "parser.add_argument(\"--dev_file\", default=\"Dev.BioC.JSON\", type=str)\n",
    "parser.add_argument(\"--test_file\", default=\"Test.BioC.JSON\", type=str)\n",
    "parser.add_argument(\"--save_path\", default=\"out\", type=str)\n",
    "parser.add_argument(\"--load_path\", default=\"./result/train_filter_cdr_tree\", type=str)\n",
    "parser.add_argument(\"--config_name\", default=\"\", type=str,\n",
    "                    help=\"Pretrained config name or path if not the same as model_name\")\n",
    "parser.add_argument(\"--tokenizer_name\", default=\"\", type=str,\n",
    "                    help=\"Pretrained tokenizer name or path if not the same as model_name\")\n",
    "parser.add_argument(\"--max_seq_length\", default=1024, type=int,\n",
    "                    help=\"The maximum total input sequence length after tokenization. Sequences longer \"\n",
    "                         \"than this will be truncated, sequences shorter will be padded.\")\n",
    "parser.add_argument(\"--train_batch_size\", default=4, type=int, help=\"Batch size for training.\")\n",
    "parser.add_argument(\"--test_batch_size\", default=8, type=int, help=\"Batch size for testing.\")\n",
    "parser.add_argument(\"--gradient_accumulation_steps\", default=1, type=int,\n",
    "                    help=\"Number of updates steps to accumulate before performing a backward/update pass.\")\n",
    "parser.add_argument(\"--learning_rate\", default=5e-5, type=float, help=\"The initial learning rate for Adam.\")\n",
    "parser.add_argument(\"--adam_epsilon\", default=1e-6, type=float, help=\"Epsilon for Adam optimizer.\")\n",
    "parser.add_argument(\"--max_grad_norm\", default=1.0, type=float, help=\"Max gradient norm.\")\n",
    "parser.add_argument(\"--warmup_ratio\", default=0.06, type=float, help=\"Warm up ratio for Adam.\")\n",
    "parser.add_argument(\"--num_train_epochs\", default=30.0, type=float, help=\"Total number of training epochs to perform.\")\n",
    "parser.add_argument(\"--evaluation_steps\", default=-1, type=int, help=\"Number of training steps between evaluations.\")\n",
    "parser.add_argument(\"--seed\", type=int, default=66, help=\"random seed for initialization\")\n",
    "parser.add_argument(\"--num_class\", type=int, default=97, help=\"Number of relation types in dataset.\")\n",
    "parser.add_argument('--gnn', type=str, default='GCN', help=\"GCN/GAT\")\n",
    "parser.add_argument('--use_gcn', type=str, default='tree', help=\"use gcn, both/mentions/tree/false\")\n",
    "parser.add_argument('--dropout', type=float, default=0.5, help=\"0.0/0.2/0.5\")\n",
    "parser.add_argument('--loss', type=str, default='BSCELoss',\n",
    "                    help=\"use BSCELoss/BalancedLoss/ATLoss/AsymmetricLoss/APLLoss\")\n",
    "parser.add_argument('--s0', type=float, default=0.3)\n",
    "parser.add_argument(\"--demo\", type=str, default='false', help='use a few data to test. default true/false')\n",
    "parser.add_argument(\"--unet_in_dim\", type=int, default=3, help=\"unet_in_dim.\")\n",
    "parser.add_argument(\"--unet_out_dim\", type=int, default=256, help=\"unet_out_dim.\")\n",
    "parser.add_argument(\"--down_dim\", type=int, default=256, help=\"down_dim.\")\n",
    "parser.add_argument(\"--bert_lr\", default=3e-5, type=float, help=\"The initial learning rate for Adam.\")\n",
    "parser.add_argument(\"--max_height\", type=int, default=64, help=\"max_height.\")\n",
    "parser.add_argument(\"--rel2\", type=int, default=0, help=\"\")\n",
    "parser.add_argument(\"--save_result\", type=str, default=\"\", help=\"save predict result.\")\n",
    "args, _ = parser.parse_known_args()\n",
    "\n",
    "if args.task == 'cdr':\n",
    "    args.data_dir = './dataset/cdr'\n",
    "    args.train_file = 'train_filter.data'\n",
    "    args.dev_file = 'dev_filter.data'\n",
    "    args.test_file = 'test_filter.data'\n",
    "    args.model_name_or_path = '/data/pretrained/scibert_scivocab_cased'\n",
    "    args.train_batch_size = 12\n",
    "    args.test_batch_size = 12\n",
    "    args.learning_rate = 2e-5\n",
    "    args.num_class = 2\n",
    "    args.num_train_epochs = 30\n",
    "\n",
    "if not os.path.exists(args.save_path):\n",
    "    os.mkdir(args.save_path)\n",
    "timestamp = datetime.datetime.now().strftime(\"%Y%m%d-%H%M\")\n",
    "file_name = \"{}_{}_{}_{}_seed_{}_{}_{}_{}_{}\".format(\n",
    "    args.train_file.split('.')[0], timestamp,\n",
    "    args.transformer_type, args.data_dir.split('/')[-1],\n",
    "    args.loss, args.use_gcn, args.s0, args.dropout, str(args.seed), )\n",
    "args.save_path = os.path.join(args.save_path, file_name)\n",
    "args.save_pubtator = os.path.join(\n",
    "    './result/' + args.task + '/' + args.task + '_' + timestamp + '_' + args.loss + '_' + str(\n",
    "        args.use_gcn) + '_s0=' + str(args.s0) + '_dropout=' + str(args.dropout) + '_' + str(args.seed))\n",
    "if args.load_path == \"\":\n",
    "    sys.stdout = Logger(stream=sys.stdout, filename=args.save_pubtator + '_test.log')\n",
    "read = read_bio\n",
    "print(args)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "args.n_gpu = torch.cuda.device_count()\n",
    "args.device = device\n",
    "config = AutoConfig.from_pretrained(\n",
    "    args.config_name if args.config_name else args.model_name_or_path, num_labels=args.num_class, )\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    args.tokenizer_name if args.tokenizer_name else args.model_name_or_path, )\n",
    "model = AutoModel.from_pretrained(\n",
    "    args.model_name_or_path, from_tf=bool(\".ckpt\" in args.model_name_or_path), config=config, )\n",
    "config.cls_token_id = tokenizer.cls_token_id\n",
    "config.sep_token_id = tokenizer.sep_token_id\n",
    "config.transformer_type = args.transformer_type\n",
    "set_seed(args)\n",
    "model = Model(args, config, model, num_labels=1)\n",
    "model.to(0)\n",
    "\n",
    "# Testing\n",
    "if len(args.save_result) < 1:\n",
    "    args.save_result = './result/' + args.task + '/'\n",
    "args.load_path = os.path.join(args.load_path)\n",
    "args.save_pubtator = os.path.join(args.save_result + args.task + '_rel2_' + str(args.rel2) + '_test')\n",
    "print(args.load_path)\n",
    "test_file = os.path.join(args.data_dir, args.test_file)\n",
    "test_features = read(args, test_file, tokenizer, max_seq_length=args.max_seq_length)\n",
    "print(\"CDR TEST\")\n",
    "model.load_state_dict(torch.load(args.load_path))\n",
    "test_score, test_output = evaluate(args, model, test_features, tag=\"test\", generate=True)\n",
    "print(test_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation on the BioRED dataset (only entity pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(adam_epsilon=1e-06, bert_lr=3e-05, config_name='', data_dir='./dataset/biored_cd', demo='false', dev_file='test.data', down_dim=256, dropout=0.5, evaluation_steps=-1, gnn='GCN', gradient_accumulation_steps=1, learning_rate=2e-05, load_path='./result/train+dev_biored_cd_both_rel2', loss='BSCELoss', max_grad_norm=1.0, max_height=64, max_seq_length=1024, model_name_or_path='/data/pretrained/BiomedNLP-PubMedBERT-base-uncased-abstract', num_class=2, num_train_epochs=50, rel2=1, s0=0.3, save_path='out/train+dev_20240818-2108_bert_biored_cd_seed_BSCELoss_tree_0.3_0.5_66', save_pubtator='./result/biored_cd/biored_cd_20240818-2108_BSCELoss_tree_s0=0.3_dropout=0.5_66', save_result='', seed=66, task='biored_cd', test_batch_size=12, test_file='test.data', tokenizer_name='', train_batch_size=12, train_file='train+dev.data', transformer_type='bert', unet_in_dim=3, unet_out_dim=256, use_gcn='tree', warmup_ratio=0.06)\n",
      "./result/train+dev_biored_cd_both_rel2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:06<00:00, 15.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BioRED_CD TEST\n",
      "generate predict result in ./result/biored_cd/biored_cd_rel2_1_test\n",
      "./result/biored_cd/biored_cd_rel2_1_test.pubtator\n",
      "{'test_F1': 81.94019657238519, 'test_P': 83.516478927666, 'test_R': 80.42327616808063}\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import sys\n",
    "import datetime\n",
    "import os\n",
    "import torch\n",
    "from transformers import AutoConfig, AutoModel, AutoTokenizer\n",
    "from model_bio import Model\n",
    "from utils import set_seed\n",
    "from prepro_bio import read_bio\n",
    "from save_result import Logger\n",
    "from evaluation import train, evaluate\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--task\", default=\"biored_cd\", type=str)\n",
    "parser.add_argument(\"--data_dir\", default=\"./dataset/cdr\", type=str)\n",
    "parser.add_argument(\"--transformer_type\", default=\"bert\", type=str)\n",
    "parser.add_argument(\"--model_name_or_path\", default=\"bert-base-cased\", type=str)\n",
    "parser.add_argument(\"--train_file\", default=\"Train.BioC.JSON\", type=str)\n",
    "parser.add_argument(\"--dev_file\", default=\"Dev.BioC.JSON\", type=str)\n",
    "parser.add_argument(\"--test_file\", default=\"Test.BioC.JSON\", type=str)\n",
    "parser.add_argument(\"--save_path\", default=\"out\", type=str)\n",
    "parser.add_argument(\"--load_path\", default=\"./result/train+dev_biored_cd_both_rel2\", type=str)\n",
    "parser.add_argument(\"--config_name\", default=\"\", type=str,\n",
    "                    help=\"Pretrained config name or path if not the same as model_name\")\n",
    "parser.add_argument(\"--tokenizer_name\", default=\"\", type=str,\n",
    "                    help=\"Pretrained tokenizer name or path if not the same as model_name\")\n",
    "parser.add_argument(\"--max_seq_length\", default=1024, type=int,\n",
    "                    help=\"The maximum total input sequence length after tokenization. Sequences longer \"\n",
    "                         \"than this will be truncated, sequences shorter will be padded.\")\n",
    "parser.add_argument(\"--train_batch_size\", default=4, type=int, help=\"Batch size for training.\")\n",
    "parser.add_argument(\"--test_batch_size\", default=8, type=int, help=\"Batch size for testing.\")\n",
    "parser.add_argument(\"--gradient_accumulation_steps\", default=1, type=int,\n",
    "                    help=\"Number of updates steps to accumulate before performing a backward/update pass.\")\n",
    "parser.add_argument(\"--learning_rate\", default=5e-5, type=float, help=\"The initial learning rate for Adam.\")\n",
    "parser.add_argument(\"--adam_epsilon\", default=1e-6, type=float, help=\"Epsilon for Adam optimizer.\")\n",
    "parser.add_argument(\"--max_grad_norm\", default=1.0, type=float, help=\"Max gradient norm.\")\n",
    "parser.add_argument(\"--warmup_ratio\", default=0.06, type=float, help=\"Warm up ratio for Adam.\")\n",
    "parser.add_argument(\"--num_train_epochs\", default=30.0, type=float, help=\"Total number of training epochs to perform.\")\n",
    "parser.add_argument(\"--evaluation_steps\", default=-1, type=int, help=\"Number of training steps between evaluations.\")\n",
    "parser.add_argument(\"--seed\", type=int, default=66, help=\"random seed for initialization\")\n",
    "parser.add_argument(\"--num_class\", type=int, default=97, help=\"Number of relation types in dataset.\")\n",
    "parser.add_argument('--gnn', type=str, default='GCN', help=\"GCN/GAT\")\n",
    "parser.add_argument('--use_gcn', type=str, default='tree', help=\"use gcn, both/mentions/tree/false\")\n",
    "parser.add_argument('--dropout', type=float, default=0.5, help=\"0.0/0.2/0.5\")\n",
    "parser.add_argument('--loss', type=str, default='BSCELoss',\n",
    "                    help=\"use BSCELoss/BalancedLoss/ATLoss/AsymmetricLoss/APLLoss\")\n",
    "parser.add_argument('--s0', type=float, default=0.3)\n",
    "parser.add_argument(\"--demo\", type=str, default='false', help='use a few data to test. default true/false')\n",
    "parser.add_argument(\"--unet_in_dim\", type=int, default=3, help=\"unet_in_dim.\")\n",
    "parser.add_argument(\"--unet_out_dim\", type=int, default=256, help=\"unet_out_dim.\")\n",
    "parser.add_argument(\"--down_dim\", type=int, default=256, help=\"down_dim.\")\n",
    "parser.add_argument(\"--bert_lr\", default=3e-5, type=float, help=\"The initial learning rate for Adam.\")\n",
    "parser.add_argument(\"--max_height\", type=int, default=64, help=\"max_height.\")\n",
    "parser.add_argument(\"--rel2\", type=int, default=1, help=\"\")\n",
    "parser.add_argument(\"--save_result\", type=str, default=\"\", help=\"save predict result.\")\n",
    "args, _ = parser.parse_known_args()\n",
    "\n",
    "if args.task == 'biored_cd':\n",
    "    args.data_dir = './dataset/biored_cd'\n",
    "    args.train_file = 'train+dev.data'\n",
    "    args.dev_file = 'test.data'\n",
    "    args.test_file = 'test.data'\n",
    "    args.model_name_or_path = '/data/pretrained/BiomedNLP-PubMedBERT-base-uncased-abstract'\n",
    "    args.train_batch_size = 12\n",
    "    args.test_batch_size = 12\n",
    "    args.learning_rate = 2e-5\n",
    "    args.num_class = 4\n",
    "    args.num_train_epochs = 50\n",
    "    if args.rel2:\n",
    "        args.train_file = 'train+dev.data'\n",
    "        args.dev_file = 'test.data'\n",
    "        args.num_class = 2\n",
    "\n",
    "if not os.path.exists(args.save_path):\n",
    "    os.mkdir(args.save_path)\n",
    "timestamp = datetime.datetime.now().strftime(\"%Y%m%d-%H%M\")\n",
    "file_name = \"{}_{}_{}_{}_seed_{}_{}_{}_{}_{}\".format(\n",
    "    args.train_file.split('.')[0], timestamp,\n",
    "    args.transformer_type, args.data_dir.split('/')[-1],\n",
    "    args.loss, args.use_gcn, args.s0, args.dropout, str(args.seed), )\n",
    "args.save_path = os.path.join(args.save_path, file_name)\n",
    "args.save_pubtator = os.path.join(\n",
    "    './result/' + args.task + '/' + args.task + '_' + timestamp + '_' + args.loss + '_' + str(\n",
    "        args.use_gcn) + '_s0=' + str(args.s0) + '_dropout=' + str(args.dropout) + '_' + str(args.seed))\n",
    "if args.load_path == \"\":\n",
    "    sys.stdout = Logger(stream=sys.stdout, filename=args.save_pubtator + '_test.log')\n",
    "read = read_bio\n",
    "print(args)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "args.n_gpu = torch.cuda.device_count()\n",
    "args.device = device\n",
    "config = AutoConfig.from_pretrained(\n",
    "    args.config_name if args.config_name else args.model_name_or_path, num_labels=args.num_class, )\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    args.tokenizer_name if args.tokenizer_name else args.model_name_or_path, )\n",
    "model = AutoModel.from_pretrained(\n",
    "    args.model_name_or_path, from_tf=bool(\".ckpt\" in args.model_name_or_path), config=config, )\n",
    "config.cls_token_id = tokenizer.cls_token_id\n",
    "config.sep_token_id = tokenizer.sep_token_id\n",
    "config.transformer_type = args.transformer_type\n",
    "set_seed(args)\n",
    "model = Model(args, config, model, num_labels=1)\n",
    "model.to(0)\n",
    "\n",
    "# Testing\n",
    "if len(args.save_result) < 1:\n",
    "    args.save_result = './result/' + args.task + '/'\n",
    "args.load_path = os.path.join(args.load_path)\n",
    "args.save_pubtator = os.path.join(args.save_result + args.task + '_rel2_' + str(args.rel2) + '_test')\n",
    "print(args.load_path)\n",
    "test_file = os.path.join(args.data_dir, args.test_file)\n",
    "test_features = read(args, test_file, tokenizer, max_seq_length=args.max_seq_length)\n",
    "print(\"BioRED_CD TEST\")\n",
    "model.load_state_dict(torch.load(args.load_path))\n",
    "test_score, test_output = evaluate(args, model, test_features, tag=\"test\", generate=True)\n",
    "print(test_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation on the BioRED dataset (all relations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(adam_epsilon=1e-06, bert_lr=3e-05, config_name='', data_dir='./dataset/biored_cd', demo='false', dev_file='test.data', down_dim=256, dropout=0.5, evaluation_steps=-1, gnn='GCN', gradient_accumulation_steps=1, learning_rate=2e-05, load_path='./result/train+dev_biored_cd_both', loss='BSCELoss', max_grad_norm=1.0, max_height=64, max_seq_length=1024, model_name_or_path='/data/pretrained/BiomedNLP-PubMedBERT-base-uncased-abstract', num_class=4, num_train_epochs=50, rel2=0, s0=0.3, save_path='out/train+dev_20240818-2109_bert_biored_cd_seed_BSCELoss_tree_0.3_0.5_66', save_pubtator='./result/biored_cd/biored_cd_20240818-2109_BSCELoss_tree_s0=0.3_dropout=0.5_66', save_result='', seed=66, task='biored_cd', test_batch_size=12, test_file='test.data', tokenizer_name='', train_batch_size=12, train_file='train+dev.data', transformer_type='bert', unet_in_dim=3, unet_out_dim=256, use_gcn='tree', warmup_ratio=0.06)\n",
      "./result/train+dev_biored_cd_both\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:06<00:00, 15.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BioRED_CD TEST\n",
      "generate predict result in ./result/biored_cd/biored_cd_rel2_0_test\n",
      "./result/biored_cd/biored_cd_rel2_0_test.pubtator\n",
      "{'test_F1': 70.46020108902368, 'test_P': 72.22221820987677, 'test_R': 68.78306514375316}\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import sys\n",
    "import datetime\n",
    "import os\n",
    "import torch\n",
    "from transformers import AutoConfig, AutoModel, AutoTokenizer\n",
    "from model_bio import Model\n",
    "from utils import set_seed\n",
    "from prepro_bio import read_bio\n",
    "from save_result import Logger\n",
    "from evaluation import train, evaluate\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--task\", default=\"biored_cd\", type=str)\n",
    "parser.add_argument(\"--data_dir\", default=\"./dataset/cdr\", type=str)\n",
    "parser.add_argument(\"--transformer_type\", default=\"bert\", type=str)\n",
    "parser.add_argument(\"--model_name_or_path\", default=\"bert-base-cased\", type=str)\n",
    "parser.add_argument(\"--train_file\", default=\"Train.BioC.JSON\", type=str)\n",
    "parser.add_argument(\"--dev_file\", default=\"Dev.BioC.JSON\", type=str)\n",
    "parser.add_argument(\"--test_file\", default=\"Test.BioC.JSON\", type=str)\n",
    "parser.add_argument(\"--save_path\", default=\"out\", type=str)\n",
    "parser.add_argument(\"--load_path\", default=\"./result/train+dev_biored_cd_both\", type=str)\n",
    "parser.add_argument(\"--config_name\", default=\"\", type=str,\n",
    "                    help=\"Pretrained config name or path if not the same as model_name\")\n",
    "parser.add_argument(\"--tokenizer_name\", default=\"\", type=str,\n",
    "                    help=\"Pretrained tokenizer name or path if not the same as model_name\")\n",
    "parser.add_argument(\"--max_seq_length\", default=1024, type=int,\n",
    "                    help=\"The maximum total input sequence length after tokenization. Sequences longer \"\n",
    "                         \"than this will be truncated, sequences shorter will be padded.\")\n",
    "parser.add_argument(\"--train_batch_size\", default=4, type=int, help=\"Batch size for training.\")\n",
    "parser.add_argument(\"--test_batch_size\", default=8, type=int, help=\"Batch size for testing.\")\n",
    "parser.add_argument(\"--gradient_accumulation_steps\", default=1, type=int,\n",
    "                    help=\"Number of updates steps to accumulate before performing a backward/update pass.\")\n",
    "parser.add_argument(\"--learning_rate\", default=5e-5, type=float, help=\"The initial learning rate for Adam.\")\n",
    "parser.add_argument(\"--adam_epsilon\", default=1e-6, type=float, help=\"Epsilon for Adam optimizer.\")\n",
    "parser.add_argument(\"--max_grad_norm\", default=1.0, type=float, help=\"Max gradient norm.\")\n",
    "parser.add_argument(\"--warmup_ratio\", default=0.06, type=float, help=\"Warm up ratio for Adam.\")\n",
    "parser.add_argument(\"--num_train_epochs\", default=30.0, type=float, help=\"Total number of training epochs to perform.\")\n",
    "parser.add_argument(\"--evaluation_steps\", default=-1, type=int, help=\"Number of training steps between evaluations.\")\n",
    "parser.add_argument(\"--seed\", type=int, default=66, help=\"random seed for initialization\")\n",
    "parser.add_argument(\"--num_class\", type=int, default=97, help=\"Number of relation types in dataset.\")\n",
    "parser.add_argument('--gnn', type=str, default='GCN', help=\"GCN/GAT\")\n",
    "parser.add_argument('--use_gcn', type=str, default='tree', help=\"use gcn, both/mentions/tree/false\")\n",
    "parser.add_argument('--dropout', type=float, default=0.5, help=\"0.0/0.2/0.5\")\n",
    "parser.add_argument('--loss', type=str, default='BSCELoss',\n",
    "                    help=\"use BSCELoss/BalancedLoss/ATLoss/AsymmetricLoss/APLLoss\")\n",
    "parser.add_argument('--s0', type=float, default=0.3)\n",
    "parser.add_argument(\"--demo\", type=str, default='false', help='use a few data to test. default true/false')\n",
    "parser.add_argument(\"--unet_in_dim\", type=int, default=3, help=\"unet_in_dim.\")\n",
    "parser.add_argument(\"--unet_out_dim\", type=int, default=256, help=\"unet_out_dim.\")\n",
    "parser.add_argument(\"--down_dim\", type=int, default=256, help=\"down_dim.\")\n",
    "parser.add_argument(\"--bert_lr\", default=3e-5, type=float, help=\"The initial learning rate for Adam.\")\n",
    "parser.add_argument(\"--max_height\", type=int, default=64, help=\"max_height.\")\n",
    "parser.add_argument(\"--rel2\", type=int, default=0, help=\"\")\n",
    "parser.add_argument(\"--save_result\", type=str, default=\"\", help=\"save predict result.\")\n",
    "args, _ = parser.parse_known_args()\n",
    "\n",
    "if args.task == 'biored_cd':\n",
    "    args.data_dir = './dataset/biored_cd'\n",
    "    args.train_file = 'train+dev.data'\n",
    "    args.dev_file = 'test.data'\n",
    "    args.test_file = 'test.data'\n",
    "    args.model_name_or_path = '/data/pretrained/BiomedNLP-PubMedBERT-base-uncased-abstract'\n",
    "    args.train_batch_size = 12\n",
    "    args.test_batch_size = 12\n",
    "    args.learning_rate = 2e-5\n",
    "    args.num_class = 4\n",
    "    args.num_train_epochs = 50\n",
    "    if args.rel2:\n",
    "        args.train_file = 'train+dev.data'\n",
    "        args.dev_file = 'test.data'\n",
    "        args.num_class = 2\n",
    "\n",
    "if not os.path.exists(args.save_path):\n",
    "    os.mkdir(args.save_path)\n",
    "timestamp = datetime.datetime.now().strftime(\"%Y%m%d-%H%M\")\n",
    "file_name = \"{}_{}_{}_{}_seed_{}_{}_{}_{}_{}\".format(\n",
    "    args.train_file.split('.')[0], timestamp,\n",
    "    args.transformer_type, args.data_dir.split('/')[-1],\n",
    "    args.loss, args.use_gcn, args.s0, args.dropout, str(args.seed), )\n",
    "args.save_path = os.path.join(args.save_path, file_name)\n",
    "args.save_pubtator = os.path.join(\n",
    "    './result/' + args.task + '/' + args.task + '_' + timestamp + '_' + args.loss + '_' + str(\n",
    "        args.use_gcn) + '_s0=' + str(args.s0) + '_dropout=' + str(args.dropout) + '_' + str(args.seed))\n",
    "if args.load_path == \"\":\n",
    "    sys.stdout = Logger(stream=sys.stdout, filename=args.save_pubtator + '_test.log')\n",
    "read = read_bio\n",
    "print(args)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "args.n_gpu = torch.cuda.device_count()\n",
    "args.device = device\n",
    "config = AutoConfig.from_pretrained(\n",
    "    args.config_name if args.config_name else args.model_name_or_path, num_labels=args.num_class, )\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    args.tokenizer_name if args.tokenizer_name else args.model_name_or_path, )\n",
    "model = AutoModel.from_pretrained(\n",
    "    args.model_name_or_path, from_tf=bool(\".ckpt\" in args.model_name_or_path), config=config, )\n",
    "config.cls_token_id = tokenizer.cls_token_id\n",
    "config.sep_token_id = tokenizer.sep_token_id\n",
    "config.transformer_type = args.transformer_type\n",
    "set_seed(args)\n",
    "model = Model(args, config, model, num_labels=1)\n",
    "model.to(0)\n",
    "\n",
    "# Testing\n",
    "if len(args.save_result) < 1:\n",
    "    args.save_result = './result/' + args.task + '/'\n",
    "args.load_path = os.path.join(args.load_path)\n",
    "args.save_pubtator = os.path.join(args.save_result + args.task + '_rel2_' + str(args.rel2) + '_test')\n",
    "print(args.load_path)\n",
    "test_file = os.path.join(args.data_dir, args.test_file)\n",
    "test_features = read(args, test_file, tokenizer, max_seq_length=args.max_seq_length)\n",
    "print(\"BioRED_CD TEST\")\n",
    "model.load_state_dict(torch.load(args.load_path))\n",
    "test_score, test_output = evaluate(args, model, test_features, tag=\"test\", generate=True)\n",
    "print(test_output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
